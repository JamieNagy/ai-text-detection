{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34695867",
      "metadata": {
        "papermill": {
          "duration": 0.004574,
          "end_time": "2024-01-21T13:30:48.667159",
          "exception": false,
          "start_time": "2024-01-21T13:30:48.662585",
          "status": "completed"
        },
        "tags": [],
        "id": "34695867"
      },
      "source": [
        "## Basic Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bfb8f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:30:48.677692Z",
          "iopub.status.busy": "2024-01-21T13:30:48.677398Z",
          "iopub.status.idle": "2024-01-21T13:31:21.618930Z",
          "shell.execute_reply": "2024-01-21T13:31:21.617971Z"
        },
        "papermill": {
          "duration": 32.953276,
          "end_time": "2024-01-21T13:31:21.624863",
          "exception": false,
          "start_time": "2024-01-21T13:30:48.671587",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "24bfb8f7",
        "outputId": "9367f7ab-9981-4a3f-df3a-493c440b1c7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Version: 2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers, sklearn, os, gc, re, random, time, sys, optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from accelerate import cpu_offload, dispatch_model\n",
        "from accelerate.utils.modeling import infer_auto_device_map\n",
        "from tqdm.auto import tqdm\n",
        "import ctypes\n",
        "libc = ctypes.CDLL(\"libc.so.6\")\n",
        "tqdm.pandas()\n",
        "\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_colwidth = 99\n",
        "\n",
        "print(f'Torch Version: {torch.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c15540f",
      "metadata": {
        "papermill": {
          "duration": 0.004594,
          "end_time": "2024-01-21T13:31:21.634008",
          "exception": false,
          "start_time": "2024-01-21T13:31:21.629414",
          "status": "completed"
        },
        "tags": [],
        "id": "0c15540f"
      },
      "source": [
        "## Imports for training on TPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3623af",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:21.644852Z",
          "iopub.status.busy": "2024-01-21T13:31:21.644417Z",
          "iopub.status.idle": "2024-01-21T13:31:22.415437Z",
          "shell.execute_reply": "2024-01-21T13:31:22.414564Z"
        },
        "papermill": {
          "duration": 0.779031,
          "end_time": "2024-01-21T13:31:22.417418",
          "exception": false,
          "start_time": "2024-01-21T13:31:21.638387",
          "status": "completed"
        },
        "tags": [],
        "id": "df3623af"
      },
      "outputs": [],
      "source": [
        "## Imports for Transformers and PEFT (Parameter-Efficient Fine-Tuning)\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import (\n",
        "    LlamaModel, LlamaConfig, LlamaForSequenceClassification, BitsAndBytesConfig,\n",
        "    AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, MistralForSequenceClassification\n",
        ")\n",
        "\n",
        "## Imports for TPU XLA\n",
        "import torch_xla.debug.profiler as xp\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import torch_xla.experimental.xla_sharding as xs\n",
        "import torch_xla.runtime as xr\n",
        "xr.use_spmd() # To enable PyTorch/XLA SPMD execution mode for automatic parallelization\n",
        "assert xr.is_spmd() == True\n",
        "\n",
        "# \"experimental\" XLA packages\n",
        "import torch_xla.experimental.xla_sharding as xs\n",
        "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
        "from torch_xla.experimental.xla_sharding import Mesh\n",
        "from spmd_util import partition_module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d41f1c",
      "metadata": {
        "papermill": {
          "duration": 0.004433,
          "end_time": "2024-01-21T13:31:22.426806",
          "exception": false,
          "start_time": "2024-01-21T13:31:22.422373",
          "status": "completed"
        },
        "tags": [],
        "id": "45d41f1c"
      },
      "source": [
        "## Common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7811678d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:22.437428Z",
          "iopub.status.busy": "2024-01-21T13:31:22.437131Z",
          "iopub.status.idle": "2024-01-21T13:31:22.444464Z",
          "shell.execute_reply": "2024-01-21T13:31:22.443747Z"
        },
        "papermill": {
          "duration": 0.014527,
          "end_time": "2024-01-21T13:31:22.446039",
          "exception": false,
          "start_time": "2024-01-21T13:31:22.431512",
          "status": "completed"
        },
        "tags": [],
        "id": "7811678d"
      },
      "outputs": [],
      "source": [
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# Seed the same seed to all\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b78df3",
      "metadata": {
        "papermill": {
          "duration": 0.004322,
          "end_time": "2024-01-21T13:31:22.455133",
          "exception": false,
          "start_time": "2024-01-21T13:31:22.450811",
          "status": "completed"
        },
        "tags": [],
        "id": "e8b78df3"
      },
      "source": [
        "# Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff31256",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:22.465505Z",
          "iopub.status.busy": "2024-01-21T13:31:22.465255Z",
          "iopub.status.idle": "2024-01-21T13:31:22.471362Z",
          "shell.execute_reply": "2024-01-21T13:31:22.470625Z"
        },
        "papermill": {
          "duration": 0.013155,
          "end_time": "2024-01-21T13:31:22.472819",
          "exception": false,
          "start_time": "2024-01-21T13:31:22.459664",
          "status": "completed"
        },
        "tags": [],
        "id": "eff31256"
      },
      "outputs": [],
      "source": [
        "# Cross validation\n",
        "def cv_split(train_data):\n",
        "    N_FOLD = 5\n",
        "    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
        "    X = train_data.loc[:, train_data.columns != \"label\"]\n",
        "    y = train_data.loc[:, train_data.columns == \"label\"]\n",
        "    # Split the train into 5 folds\n",
        "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
        "        train_data.loc[valid_index, \"fold\"] = fold\n",
        "\n",
        "    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
        "    return train_data\n",
        "\n",
        "def load_train_data():\n",
        "    train_data = pd.read_csv('/kaggle/input/llm-text-detection-deverta-small-lr7e-5/df_all.csv')\n",
        "    train_data = train_data[[\"text\", \"labels\"]]\n",
        "    train_data = train_data.rename(columns={'labels':'label'})\n",
        "    train_data.reset_index(inplace=True, drop=True)\n",
        "    #print(f\"Train data has shape: {train_data.shape}\")\n",
        "    #print(train_data.head())\n",
        "    print(f\"Train data value counts: {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
        "    return train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3968d7ce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:22.483145Z",
          "iopub.status.busy": "2024-01-21T13:31:22.482910Z",
          "iopub.status.idle": "2024-01-21T13:31:27.276374Z",
          "shell.execute_reply": "2024-01-21T13:31:27.275543Z"
        },
        "papermill": {
          "duration": 4.80087,
          "end_time": "2024-01-21T13:31:27.278028",
          "exception": false,
          "start_time": "2024-01-21T13:31:22.477158",
          "status": "completed"
        },
        "tags": [],
        "id": "3968d7ce",
        "outputId": "4591256f-0131-4cea-edce-327e98b1249a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data value counts: label\n",
            "0    53958\n",
            "1    36797\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How often do you ride in a car? Do you drive a one or any other motor vehicle to work? The stor...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cars are a wonderful thing. They are perhaps one of the worlds greatest advancements and techno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                 text  \\\n",
              "0  Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...   \n",
              "1  Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...   \n",
              "2  \"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...   \n",
              "3  How often do you ride in a car? Do you drive a one or any other motor vehicle to work? The stor...   \n",
              "4  Cars are a wonderful thing. They are perhaps one of the worlds greatest advancements and techno...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data = load_train_data()\n",
        "display(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaefc99f",
      "metadata": {
        "papermill": {
          "duration": 0.004924,
          "end_time": "2024-01-21T13:31:27.313785",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.308861",
          "status": "completed"
        },
        "tags": [],
        "id": "aaefc99f"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9297e860",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:27.325333Z",
          "iopub.status.busy": "2024-01-21T13:31:27.325069Z",
          "iopub.status.idle": "2024-01-21T13:31:27.360726Z",
          "shell.execute_reply": "2024-01-21T13:31:27.359906Z"
        },
        "papermill": {
          "duration": 0.044076,
          "end_time": "2024-01-21T13:31:27.362404",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.318328",
          "status": "completed"
        },
        "tags": [],
        "id": "9297e860"
      },
      "outputs": [],
      "source": [
        "class TrainModelTPU():\n",
        "    def __init__(self, model, train_data, **params):\n",
        "        # Create train and valid dataset\n",
        "        self.train_df, self.valid_df = train_test_split(train_data, test_size=0.0005,\n",
        "                                                        stratify=train_data['label'],\n",
        "                                                        random_state=SEED)\n",
        "        self.LR = params['lr'] # Learning rate\n",
        "        self.R = params['r'] # 'r' value for Lora layer\n",
        "        self.NUM_EPOCHS = params['num_epochs'] # Training Epoch\n",
        "        # Fixed parameters\n",
        "        self.NUM_LABELS = 1 # Total Number of Labels (0:human texts, 1:LLM generated texts)\n",
        "        self.MAX_LENGTH = params['max_length']\n",
        "        self.BATCH_SIZE = 16\n",
        "        self.DEVICE = xm.xla_device() # Initialize TPU Device\n",
        "        self.NUM_WARMUP_STEPS = 0 # Number of Warmup Steps\n",
        "        self.GRADIENT_ACCUMULATION_STEPS = 2\n",
        "        # The model\n",
        "        self.MODEL = model\n",
        "\n",
        "    # Load pretrained LLM and tokenizer\n",
        "    def load_model(self):\n",
        "        if \"mistral_7b\" == self.MODEL:\n",
        "            MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"  # Mistral\n",
        "        if \"llama-2_7b\" == self.MODEL:\n",
        "            MDEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-hf/1\"  # llama\n",
        "        # Load the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        # `bfloat16` is suitable for deep learning for better convergences during training\n",
        "        base_model = LlamaForSequenceClassification.from_pretrained(MODEL_PATH,\n",
        "                                                                num_labels=self.NUM_LABELS,\n",
        "                                                                torch_dtype=torch.bfloat16)\n",
        "        # No idea why this is needed\n",
        "        base_model.config.pretraining_tp = 1 # 1 is 7b\n",
        "        # Assign Padding TOKEN\n",
        "        base_model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "        # print(base_model)\n",
        "\n",
        "        # LoRa\n",
        "        peft_config = LoraConfig(\n",
        "            r=self.R,  # Use larger 'r' value increase more parameters during training\n",
        "            lora_dropout=0.001,\n",
        "            bias='none',\n",
        "            inference_mode=False,\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            # Only Use Output and Values Projection\n",
        "            target_modules=['o_proj', 'v_proj'], # layer names for llama 2 model\n",
        "        )\n",
        "        # Continue training on previous epoch\n",
        "        # Load the new PEFT model\n",
        "        self.model = get_peft_model(base_model, peft_config)\n",
        "        # Display Trainable Parameters to make sure we load the model successfully\n",
        "        self.model.print_trainable_parameters()\n",
        "        print(\"Complete loading pretrained LLM model\")\n",
        "\n",
        "    # Save the trained model as output files\n",
        "    def save_model(self):\n",
        "        self.model = self.model.cpu()# Move model first on CPU before saving weights\n",
        "        # Model saving path\n",
        "        SAVE_PATH = f'/kaggle/working/{self.MODEL}/{self.MODEL}_TPU/'\n",
        "        self.model.save_pretrained(SAVE_PATH) # Save the entire fine-tuned model\n",
        "        # Save tokenizer for inference\n",
        "        self.tokenizer.save_pretrained(SAVE_PATH)\n",
        "        # Only saving the newly trained weights\n",
        "        torch.save(dict([(k,v) for k, v in self.model.named_parameters() if v.requires_grad]),\n",
        "                   SAVE_PATH + 'model_weights.pth')\n",
        "        print(f\"Save the model and tokenizers to {SAVE_PATH}\")\n",
        "\n",
        "    # Disply trainable layers of LLM\n",
        "    def display_model_layers(self):\n",
        "        # Dispaly trainable layers for verification\n",
        "        trainable_layers = []\n",
        "        n_trainable_params = 0\n",
        "        for name, param in self.model.named_parameters():\n",
        "            # Layer Parameter Count\n",
        "            n_params = int(torch.prod(torch.tensor(param.shape)))\n",
        "            # Only Trainable Layers\n",
        "            if param.requires_grad:\n",
        "                # Add Layer Information\n",
        "                trainable_layers.append({\n",
        "                    '#param': n_params,\n",
        "                    'name': name,\n",
        "                    'dtype': param.data.dtype,\n",
        "                    'params': param\n",
        "                })\n",
        "                n_trainable_params += n_params\n",
        "\n",
        "        display(pd.DataFrame(trainable_layers))\n",
        "        print(f\"Number of trainable parameters: {n_trainable_params:,} \"\n",
        "              f\"Number of trainable layers: {len(trainable_layers)}\")\n",
        "\n",
        "    def create_optimizer_scheduler(self, STEPS_PER_EPOCH):\n",
        "        # Optimizer (Adam)\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.LR, weight_decay=0.01)\n",
        "        # Cosine Learning Rate With Warmup\n",
        "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
        "                                    optimizer=optimizer,\n",
        "                                    num_warmup_steps=self.NUM_WARMUP_STEPS,\n",
        "                                    num_training_steps=STEPS_PER_EPOCH * self.NUM_EPOCHS)\n",
        "        # Set the data type for the optimizer's state (e.g., momentum buffers)\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
        "                    state[v] = v.to(dtype=torch.float32)\n",
        "        print(\"Complete creating optimizer and lr scheduler\")\n",
        "        print(\"optimizer\", optimizer)\n",
        "        print(\"lr_scheduler\", lr_scheduler)\n",
        "        return optimizer, lr_scheduler\n",
        "\n",
        "    def partition_mesh(self):\n",
        "        # Number of TPU Nodes to ensure we can access TPUs and partition the model into mesh\n",
        "        num_devices = xr.global_runtime_device_count()\n",
        "        mesh_shape = (1, num_devices, 1)\n",
        "        print(f'Number_DEVICES: {num_devices}')\n",
        "        device_ids = np.array(range(num_devices))\n",
        "        mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
        "        partition_module(self.model, mesh)\n",
        "        return num_devices, mesh\n",
        "\n",
        "     # Create a training dataset\n",
        "    def create_dataset(self, N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, mesh):\n",
        "        IDXS = np.arange(N_SAMPLES-(N_SAMPLES%self.BATCH_SIZE))\n",
        "        while True:\n",
        "            # Shuffle Indices\n",
        "            np.random.shuffle(IDXS)\n",
        "            # Iterate Over All Indices Once\n",
        "            for idxs in IDXS.reshape(-1, self.BATCH_SIZE):\n",
        "                input_ids = torch.tensor(INPUT_IDS[idxs]).to(self.DEVICE)\n",
        "                attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(self.DEVICE)\n",
        "                labels = torch.tensor(GENERATED[idxs]).to(self.DEVICE)\n",
        "                # Shard Over TPU Nodes\n",
        "                xs.mark_sharding(input_ids, mesh, (0, 1))\n",
        "                xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
        "                xs.mark_sharding(labels, mesh, (0, 1))\n",
        "                yield input_ids, attention_mask, labels\n",
        "\n",
        "    # Validate the model\n",
        "    def valid_model(self):\n",
        "        num_devices, mesh = self.partition_mesh()\n",
        "        # Compute total samples and number of steps in one epochs\n",
        "        N_SAMPLES = len(self.valid_df)\n",
        "        print(f\"Start validating the model with number of sample {N_SAMPLES}\")\n",
        "        # Tokenize Data\n",
        "        tokens = self.tokenizer(self.valid_df['text'].tolist(), # Texts\n",
        "                                padding='max_length',       # Pad texts to maximum length\n",
        "                                max_length=self.MAX_LENGTH, # Maximum token length\n",
        "                                truncation=True,            # Truncate texts if they are too long\n",
        "                                return_tensors='np',        # Return pytorch tensor\n",
        "                           )\n",
        "\n",
        "        # Input IDs are the token IDs\n",
        "        INPUT_IDS = tokens['input_ids']\n",
        "        # Attention Masks to Ignore Padding Tokens\n",
        "        ATTENTION_MASKS = tokens['attention_mask']\n",
        "        # Generated By AI Label of Texts\n",
        "        GENERATED = self.valid_df['label'].values.reshape(-1,1).astype(np.float32)\n",
        "        # Create a valid dataset\n",
        "        VALID_DATASET = self.create_dataset(N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, mesh)\n",
        "\n",
        "        # Compute the number of batches\n",
        "        IDXS = np.array_split(np.arange(N_SAMPLES), max(1, N_SAMPLES // self.BATCH_SIZE))\n",
        "        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n",
        "        METRICS = {'loss': [],\n",
        "                   'auc': {'y_true': [], 'y_pred': []} }\n",
        "        STEPS = N_SAMPLES // self.BATCH_SIZE\n",
        "        for step in tqdm(range(STEPS)):\n",
        "            # Enable inference mode using `no_grad`\n",
        "            with torch.no_grad():\n",
        "                # Get Batch\n",
        "                input_ids, attention_mask, labels = next(VALID_DATASET)\n",
        "                 # Forward Pass\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                # Logits Float32\n",
        "                logits = outputs.logits.to(dtype=torch.float32)\n",
        "                # Backward Pass\n",
        "                loss = LOSS_FN(logits, labels)\n",
        "                # Update Metrics And Progress Bar\n",
        "                METRICS['loss'].append(float(loss))\n",
        "                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n",
        "                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n",
        "                # print(f\"Complete updating metrics for Step {step} in {time.time() - start: .1f} seconds\")\n",
        "        loss = np.mean(METRICS['loss'])\n",
        "        roc_auc = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n",
        "        # Compute and display the validation results\n",
        "        print(f\"Number of validation data {len(self.valid_df)}\\n\"\n",
        "              f\"µ_loss: {loss: .3f}\\n\"\n",
        "              f\"µ_auc: {roc_auc:.3f}\")\n",
        "        return {\"eval_loss\": loss, \"eval_roc_auc\": roc_auc}\n",
        "\n",
        "    # Train the model by the fold data\n",
        "    def train_model(self):\n",
        "        num_devices, mesh = self.partition_mesh()\n",
        "        print(f'Number_DEVICES: {num_devices}')\n",
        "        print(f\"Total number of train data = {len(self.train_df)}\")\n",
        "        # limited to two columns\n",
        "        train_df = self.train_df[['text', 'label']]\n",
        "        # Preprocess the text\n",
        "        train_df['text'] = train_df['text'].map(lambda text: pre_processing_text(text))\n",
        "        # Compute total samples and number of steps in one epochs\n",
        "        N_SAMPLES = len(train_df)\n",
        "        # Compute the total steps per epochs\n",
        "        STEPS_PER_EPOCH = N_SAMPLES // self.BATCH_SIZE\n",
        "        print(f'BATCH_SIZE: {self.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n",
        "\n",
        "        # Tokenize Data\n",
        "        tokens = self.tokenizer(train_df['text'].tolist(), # Texts\n",
        "                                padding='max_length', # Pad texts to maximum length\n",
        "                                max_length=self.MAX_LENGTH, # Maximum token length\n",
        "                                truncation=True, # Truncate texts if they are too long\n",
        "                                return_tensors='np', # Return Numpy array\n",
        "                                )\n",
        "        # Input IDs are the token IDs\n",
        "        INPUT_IDS = tokens['input_ids']\n",
        "        # Attention Masks to Ignore Padding Tokens\n",
        "        ATTENTION_MASKS = tokens['attention_mask']\n",
        "        # Generated By AI Label of Texts\n",
        "        GENERATED = train_df['label'].values.reshape(-1,1).astype(np.float32)\n",
        "        print(f'INPUT_IDS shape: {INPUT_IDS.shape}\\n'\n",
        "              f'ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}\\n'\n",
        "              f'GENERATED shape: {GENERATED.shape}')\n",
        "\n",
        "        # Create a train dataset\n",
        "        TRAIN_DATASET = self.create_dataset(N_SAMPLES, INPUT_IDS, ATTENTION_MASKS, GENERATED, mesh)\n",
        "\n",
        "        # Create optimizer and lr_scheduler\n",
        "        optimizer, lr_scheduler = self.create_optimizer_scheduler(STEPS_PER_EPOCH)\n",
        "\n",
        "        # Put Model In Train Modus\n",
        "        self.model.train()\n",
        "        # Loss Function, basic Binary Cross Entropy\n",
        "        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n",
        "        eval_scores = []\n",
        "        # Training loop goes through each epoch\n",
        "        for epoch in tqdm(range(self.NUM_EPOCHS)):\n",
        "            start = time.time()\n",
        "            METRICS = {'loss': [],\n",
        "                       'auc': {'y_true': [], 'y_pred': []} }\n",
        "            # Go through each step\n",
        "            for step in range(STEPS_PER_EPOCH):\n",
        "                # Zero Out Gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Get Batch\n",
        "                input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
        "                # Test the TRAIN_DATASET for debugging first record\n",
        "                # Forward Pass\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                # Logits Float32\n",
        "                logits = outputs.logits.to(dtype=torch.float32)\n",
        "                # Backward Pass\n",
        "                loss = LOSS_FN(logits, labels)\n",
        "                # backward propagation pass\n",
        "                loss.backward()\n",
        "                if (step + 1) % self.GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    # Update Weights\n",
        "                    optimizer.step()\n",
        "                    xm.mark_step()\n",
        "                    # Update Learning Rate Scheduler\n",
        "                    lr_scheduler.step()\n",
        "                # Update Metrics And Progress Bar\n",
        "                METRICS['loss'].append(float(loss))\n",
        "                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n",
        "                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n",
        "                # print(f\"Complete updating metrics {METRICS}\")\n",
        "                # Metrics Shown After Both Classes Present\n",
        "                if np.unique(METRICS['auc']['y_true']).size == 2:\n",
        "                    metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
        "                    metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
        "                    metrics += ', µ_auc: {:.3f}'.format(\n",
        "                        sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n",
        "                    )\n",
        "\n",
        "                    lr = optimizer.param_groups[0]['lr']\n",
        "                    print('\\r'*100, f'{epoch+1:02}/{self.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
        "            avg_loss = np.mean(METRICS['loss'])\n",
        "            roc_auc_score = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'],\n",
        "                                                          METRICS['auc']['y_pred'])\n",
        "            print(f'\\n=== Finish Training Epoch {epoch} with average loss {avg_loss: .5f} '\n",
        "                  f'ROC Accuracy Score {roc_auc_score:.5f} ===')\n",
        "            # Validate the model at the end of epochs\n",
        "            result = self.valid_model()\n",
        "            eval_roc_auc = float(result['eval_roc_auc'])\n",
        "            print(f'\\n=== Finish Validating the model with evaluated ROC Accuracy Score {eval_roc_auc:.5f}'\n",
        "                  f'\\n Total running time = {time.time() -  start: .1f} seconds ===')\n",
        "            eval_scores.append(eval_roc_auc)\n",
        "        return np.mean(eval_scores)\n",
        "\n",
        "    # Clear the memory\n",
        "    def clear_memory(self):\n",
        "        del self.model, self.tokenizer\n",
        "        libc.malloc_trim(0)\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b1789e",
      "metadata": {
        "papermill": {
          "duration": 0.004903,
          "end_time": "2024-01-21T13:31:27.371837",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.366934",
          "status": "completed"
        },
        "tags": [],
        "id": "56b1789e"
      },
      "source": [
        "# Use Optuna to find the optimal hyper-parameters\n",
        "Train and Save the model with best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ca6097",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:27.382852Z",
          "iopub.status.busy": "2024-01-21T13:31:27.382573Z",
          "iopub.status.idle": "2024-01-21T13:31:27.389989Z",
          "shell.execute_reply": "2024-01-21T13:31:27.389164Z"
        },
        "papermill": {
          "duration": 0.015327,
          "end_time": "2024-01-21T13:31:27.391698",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.376371",
          "status": "completed"
        },
        "tags": [],
        "id": "f2ca6097",
        "outputId": "1b27ece9-603b-4f67-a960-0e97529d02b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                 text  \\\n",
              "0  Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...   \n",
              "1  Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...   \n",
              "2  \"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(train_data.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62157ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:27.403081Z",
          "iopub.status.busy": "2024-01-21T13:31:27.402606Z",
          "iopub.status.idle": "2024-01-21T13:31:27.409447Z",
          "shell.execute_reply": "2024-01-21T13:31:27.408702Z"
        },
        "papermill": {
          "duration": 0.014848,
          "end_time": "2024-01-21T13:31:27.411206",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.396358",
          "status": "completed"
        },
        "tags": [],
        "id": "c62157ac"
      },
      "outputs": [],
      "source": [
        "# Start optuna study to hyper-parameter tuning\n",
        "best_score = -1.0\n",
        "# Find the optimal learning rate\n",
        "def objective(trial, model_name, train_data):\n",
        "    # Parameters\n",
        "    params = {\n",
        "        'lr': trial.suggest_float('learning_rate', 1e-7, 1e-3, log=True),\n",
        "        'r': 64, # Default: 64\n",
        "        'num_epochs': 1,\n",
        "        'max_length' : 512,\n",
        "    }\n",
        "    # Create a trainer\n",
        "    trainer = TrainModelTPU(model_name, train_data, **params)\n",
        "    trainer.load_model()\n",
        "    eval_score = trainer.train_model()\n",
        "    # Save the model is the avg score > current best score\n",
        "    global best_score\n",
        "    if eval_score > best_score:\n",
        "        best_score = eval_score\n",
        "        # Save all the fold models\n",
        "        trainer.save_model()\n",
        "    # Clean up\n",
        "    trainer.clear_memory()\n",
        "    del trainer\n",
        "    return eval_score  # Maximal the average 'roc_auc' metric\n",
        "\n",
        "def train_model_with_optuna(model_name, train_data):\n",
        "    # # Create a study to find the optimal hyper-parameters\\\n",
        "    study_name = f\"{model_name}_study\"\n",
        "    study_file = f\"/kaggle/working/{study_name}.db\"\n",
        "    # Delete the study file if exits\n",
        "    if os.path.isfile(study_file):\n",
        "        os.remove(f'{study_file}')\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=study_name,\n",
        "                                storage=\"sqlite:///\" + f\"{study_file}\", # Storage path of the database keeping the study results\n",
        "                                load_if_exists=False) # True: Resume the study, False: Createa new one\n",
        "    # Set up the timeout to avoid runing out of quote\n",
        "    study.optimize(lambda trial: objective(trial, model_name, train_data),\n",
        "                   timeout=600, n_jobs=1, n_trials=10,\n",
        "                   show_progress_bar=True, gc_after_trial=True)\n",
        "    print(f\"Best parameters: {study.best_params}\")\n",
        "    params = study.best_params # Obtain the optimal parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872a61bf",
      "metadata": {
        "papermill": {
          "duration": 0.029734,
          "end_time": "2024-01-21T13:31:27.445734",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.416000",
          "status": "completed"
        },
        "tags": [],
        "id": "872a61bf"
      },
      "source": [
        "# Train the model with best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "696c0a8a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:27.456864Z",
          "iopub.status.busy": "2024-01-21T13:31:27.456608Z",
          "iopub.status.idle": "2024-01-21T13:31:27.461330Z",
          "shell.execute_reply": "2024-01-21T13:31:27.460567Z"
        },
        "papermill": {
          "duration": 0.01225,
          "end_time": "2024-01-21T13:31:27.462906",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.450656",
          "status": "completed"
        },
        "tags": [],
        "id": "696c0a8a"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, train_data):\n",
        "    # Parameters\n",
        "    params = {\n",
        "        'lr': 5e-5, # learning rate\n",
        "        'r': 64, # Lora's r value (Default: 64)\n",
        "        'num_epochs': 2, # number of epochs\n",
        "        'max_length': 512\n",
        "    }\n",
        "    # Create a trainer\n",
        "    trainer = TrainModelTPU(model_name, train_data, **params)\n",
        "    trainer.load_model()# Load the pretrained LLM and tokenizer\n",
        "    eval_score = trainer.train_model()\n",
        "    # Save the model is the avg score > current best score\n",
        "    # Save all the fold models\n",
        "    trainer.save_model()\n",
        "    # Clean up\n",
        "    trainer.clear_memory()\n",
        "    del trainer\n",
        "    print(f\"=== Finish training the model {model_name} with score = {eval_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2b71a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:31:27.473940Z",
          "iopub.status.busy": "2024-01-21T13:31:27.473691Z",
          "iopub.status.idle": "2024-01-21T21:26:55.019513Z",
          "shell.execute_reply": "2024-01-21T21:26:55.018362Z"
        },
        "papermill": {
          "duration": 28527.55396,
          "end_time": "2024-01-21T21:26:55.021481",
          "exception": false,
          "start_time": "2024-01-21T13:31:27.467521",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "dd2b71a4",
        "outputId": "190a5679-b97f-4fea-addf-7a798d7ddb29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [01:34<00:00, 47.18s/it]\n",
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 27,267,072 || all params: 7,137,931,264 || trainable%: 0.3820024456878827\n",
            "Complete loading pretrained LLM model\n",
            "Number_DEVICES: 8\n",
            "Number_DEVICES: 8\n",
            "Total number of train data = 90709\n",
            "BATCH_SIZE: 16, N_SAMPLES: 90709, STEPS_PER_EPOCH: 5669\n",
            "INPUT_IDS shape: (90709, 512)\n",
            "ATTENTION_MASKS shape: (90709, 512)\n",
            "GENERATED shape: (90709, 1)\n",
            "Complete creating optimizer and lr scheduler\n",
            "optimizer AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 5e-05\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "lr_scheduler <torch.optim.lr_scheduler.LambdaLR object at 0x7f1f22ed04c0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 01/02 | 5669/5669 lr: 4.27E-05, µ_loss: 0.058, step_loss: 0.005, µ_auc: 0.998\n",
            "=== Finish Training Epoch 0 with average loss  0.05764 ROC Accuracy Score 0.99848 ===\n",
            "Number_DEVICES: 8\n",
            "Start validating the model with number of sample 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:51<00:51, 51.20s/it]\u001b[A\n",
            "100%|██████████| 2/2 [01:43<00:00, 51.97s/it]\n",
            " 50%|█████     | 1/2 [2:59:47<2:59:47, 10787.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of validation data 46\n",
            "µ_loss:  0.000\n",
            "µ_auc: 1.000\n",
            "\n",
            "=== Finish Validating the model with evaluated ROC Accuracy Score 1.00000\n",
            " Total running time =  10787.3 seconds ===\n",
            " 02/02 | 5669/5669 lr: 2.50E-05, µ_loss: 0.016, step_loss: 0.000, µ_auc: 1.000\n",
            "=== Finish Training Epoch 1 with average loss  0.01555 ROC Accuracy Score 0.99971 ===\n",
            "Number_DEVICES: 8\n",
            "Start validating the model with number of sample 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:01<00:01,  1.38s/it]\u001b[A\n",
            "100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n",
            "100%|██████████| 2/2 [5:59:43<00:00, 10791.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of validation data 46\n",
            "µ_loss:  0.000\n",
            "µ_auc: 1.000\n",
            "\n",
            "=== Finish Validating the model with evaluated ROC Accuracy Score 1.00000\n",
            " Total running time =  10796.4 seconds ===\n",
            "Save the model and tokenizers to /kaggle/working/mistral_7b/mistral_7b_TPU/\n",
            "=== Finish training the model mistral_7b with score = 1.0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"mistral_7b\" # \"mistral_7b\"\n",
        "#train_model_with_optuna(model_name, train_data)\n",
        "train_model(model_name, train_data)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "databundleVersionId": 7516023,
          "sourceId": 61542,
          "sourceType": "competition"
        },
        {
          "datasetId": 2663421,
          "sourceId": 4620664,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3863727,
          "sourceId": 6703755,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3972872,
          "sourceId": 6921012,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4005256,
          "sourceId": 6977472,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4287904,
          "sourceId": 7378735,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4328487,
          "sourceId": 7437171,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4059536,
          "sourceId": 7443031,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4049286,
          "sourceId": 7443707,
          "sourceType": "datasetVersion"
        },
        {
          "modelInstanceId": 3090,
          "sourceId": 4295,
          "sourceType": "modelInstanceVersion"
        },
        {
          "modelInstanceId": 3899,
          "sourceId": 5111,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30581,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 28632.630084,
      "end_time": "2024-01-21T21:27:11.703642",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-01-21T13:29:59.073558",
      "version": "2.5.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}