{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8758a98b",
      "metadata": {
        "papermill": {
          "duration": 0.007674,
          "end_time": "2024-01-22T18:54:25.785186",
          "exception": false,
          "start_time": "2024-01-22T18:54:25.777512",
          "status": "completed"
        },
        "tags": [],
        "id": "8758a98b"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f0ea60",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:54:25.802396Z",
          "iopub.status.busy": "2024-01-22T18:54:25.802072Z",
          "iopub.status.idle": "2024-01-22T18:54:43.488030Z",
          "shell.execute_reply": "2024-01-22T18:54:43.487078Z"
        },
        "papermill": {
          "duration": 17.697161,
          "end_time": "2024-01-22T18:54:43.490145",
          "exception": false,
          "start_time": "2024-01-22T18:54:25.792984",
          "status": "completed"
        },
        "tags": [],
        "id": "47f0ea60",
        "outputId": "ebca35f4-ea8d-473e-91c4-34b971621ba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Version: 2.0.0\n",
            "4.35.0\n",
            "0.6.0\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "import transformers\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from transformers import (\n",
        "    Trainer, DataCollatorWithPadding, BitsAndBytesConfig, AutoTokenizer,\n",
        "    LlamaForSequenceClassification, DataCollatorWithPadding, Trainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "\n",
        "print(f'Torch Version: {torch.__version__}')\n",
        "print(transformers.__version__)\n",
        "print(peft.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8f04cc",
      "metadata": {
        "papermill": {
          "duration": 0.007788,
          "end_time": "2024-01-22T18:54:43.528988",
          "exception": false,
          "start_time": "2024-01-22T18:54:43.521200",
          "status": "completed"
        },
        "tags": [],
        "id": "ae8f04cc"
      },
      "source": [
        "# Mistral Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c5fb39",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:54:43.545822Z",
          "iopub.status.busy": "2024-01-22T18:54:43.545575Z",
          "iopub.status.idle": "2024-01-22T18:54:43.557246Z",
          "shell.execute_reply": "2024-01-22T18:54:43.556457Z"
        },
        "papermill": {
          "duration": 0.022382,
          "end_time": "2024-01-22T18:54:43.559101",
          "exception": false,
          "start_time": "2024-01-22T18:54:43.536719",
          "status": "completed"
        },
        "tags": [],
        "id": "08c5fb39"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned Mistral model\n",
        "class MistralModelInference:\n",
        "    def __init__(self, model_path, adapter_path):\n",
        "        self.MODEL_PATH = model_path  # The path to Mistral model\n",
        "        self.ADAPTER_PATH = adapter_path # The path to the fine-tuned adapter\n",
        "        self.NUM_LABELS = 1\n",
        "        self.MAX_LENGTH = 512\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        start = time.time()\n",
        "        # Load the tokenizer of LLM model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL_PATH, use_fast=False)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "         # set the pad token of the model's configuration\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "        # Load base/pretrained LLM model\n",
        "        base_model = LlamaForSequenceClassification.from_pretrained(self.MODEL_PATH,\n",
        "                                                                    num_labels=self.NUM_LABELS,\n",
        "                                                                    quantization_config=bnb_config,\n",
        "                                                                    )\n",
        "\n",
        "        # No idea why this is needed\n",
        "        base_model.config.pretraining_tp = 1  # 1 is 7b\n",
        "        # Assign Padding TOKEN\n",
        "        base_model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        # Load the fine-tuned adapter layer on top of base model\n",
        "        self.model = PeftModel.from_pretrained(base_model, self.ADAPTER_PATH)\n",
        "        print(f\"Complete loading pretrained LLM model {time.time() - start:.1f} seconds\")\n",
        "\n",
        "    def preprocess_function(self, examples):\n",
        "        examples[\"text\"] = list(map(lambda text: pre_processing_text(text), examples[\"text\"]))\n",
        "        return self.tokenizer(examples[\"text\"], truncation=True,\n",
        "                              max_length=self.MAX_LENGTH, padding=True)\n",
        "    # Map x to 0 to 1.\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def inference(self, test_texts):\n",
        "        test_data = pd.DataFrame({'text': test_texts})\n",
        "        test_dataset = Dataset.from_pandas(test_data)\n",
        "        test_tokenized_ds = test_dataset.map(self.preprocess_function, batched=True)\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer, padding=\"longest\")\n",
        "        trainer = Trainer(model=self.model,\n",
        "                          tokenizer=self.tokenizer,\n",
        "                          data_collator=data_collator)\n",
        "\n",
        "        pred_output = trainer.predict(test_tokenized_ds)\n",
        "        logits = pred_output.predictions\n",
        "        print(logits)\n",
        "        predicted_probs = self.sigmoid(logits[:, 0]) # Get the probability of texts generated by LLMs\n",
        "        return predicted_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbfbe28",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:54:43.577447Z",
          "iopub.status.busy": "2024-01-22T18:54:43.576926Z",
          "iopub.status.idle": "2024-01-22T18:57:08.940176Z",
          "shell.execute_reply": "2024-01-22T18:57:08.939235Z"
        },
        "papermill": {
          "duration": 145.375634,
          "end_time": "2024-01-22T18:57:08.943652",
          "exception": false,
          "start_time": "2024-01-22T18:54:43.568018",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "0bd76614bc63468d992c97fd67cd9a4b",
            "f7b5fb60851d45839982d13eeb08afc1"
          ]
        },
        "id": "cdbfbe28",
        "outputId": "0024f71b-1446-4f48-83f9-fe8995146679"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bd76614bc63468d992c97fd67cd9a4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete loading pretrained LLM model 143.6 seconds\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7b5fb60851d45839982d13eeb08afc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[12.47]\n",
            " [10.56]]\n",
            "Predicted Probabilities: [1. 1.]\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/mistral/pytorch/7b-v0.1-hf/1\"  # Mistral\"\n",
        "# Adapter path stores the fine-tuned adapter, generated from the notebook to improve Mistral model's performance\n",
        "adpater_path = \"/mistral-7b-tpu-trained-checkpoint/mistral_7b/mistral_7b_TPU\"\n",
        "mistral_inference = MistralModelInference(model_path, adpater_path)\n",
        "\n",
        "test_texts = [\"Your test text goes here.\", \"Another test text.\"]\n",
        "\n",
        "# Perform inference\n",
        "predicted_probs = mistral_inference.inference(test_texts)\n",
        "\n",
        "print(\"Predicted Probabilities:\", predicted_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039bf86c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:08.981144Z",
          "iopub.status.busy": "2024-01-22T18:57:08.980825Z",
          "iopub.status.idle": "2024-01-22T18:57:09.002308Z",
          "shell.execute_reply": "2024-01-22T18:57:09.001166Z"
        },
        "papermill": {
          "duration": 0.033526,
          "end_time": "2024-01-22T18:57:09.004543",
          "exception": false,
          "start_time": "2024-01-22T18:57:08.971017",
          "status": "completed"
        },
        "tags": [],
        "id": "039bf86c",
        "outputId": "51e599d5-3625-4d03-9f50-4f7c718e7bef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000aaaa</td>\n",
              "      <td>2</td>\n",
              "      <td>Aaa bbb ccc.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1111bbbb</td>\n",
              "      <td>3</td>\n",
              "      <td>Bbb ccc ddd.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2222cccc</td>\n",
              "      <td>4</td>\n",
              "      <td>CCC ddd eee.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  prompt_id          text\n",
              "0  0000aaaa          2  Aaa bbb ccc.\n",
              "1  1111bbbb          3  Bbb ccc ddd.\n",
              "2  2222cccc          4  CCC ddd eee."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load test data\n",
        "test_df = pd.read_csv(\"/llm-detect-ai-generated-text/test_essays.csv\", sep=',')\n",
        "test_df = test_df.rename(columns={'generated': 'label'})\n",
        "\n",
        "display(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec99efc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:09.024053Z",
          "iopub.status.busy": "2024-01-22T18:57:09.023712Z",
          "iopub.status.idle": "2024-01-22T18:57:09.630337Z",
          "shell.execute_reply": "2024-01-22T18:57:09.629409Z"
        },
        "papermill": {
          "duration": 0.618788,
          "end_time": "2024-01-22T18:57:09.632700",
          "exception": false,
          "start_time": "2024-01-22T18:57:09.013912",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "1b2e2f9941934cf7a88ef1858758e48f"
          ]
        },
        "id": "8ec99efc",
        "outputId": "6dccd3c5-93c7-4ac7-9746-795df8ec9245"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b2e2f9941934cf7a88ef1858758e48f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[6.332]\n",
            " [1.454]\n",
            " [7.03 ]]\n",
            "ID 0000aaaa, prob = 0.998046875\n",
            "ID 1111bbbb, prob = 0.810546875\n",
            "ID 2222cccc, prob = 0.9990234375\n",
            "predictions = [{'id': '0000aaaa', 'generated': 0.998}, {'id': '1111bbbb', 'generated': 0.8105}, {'id': '2222cccc', 'generated': 0.999}]\n"
          ]
        }
      ],
      "source": [
        "# Infer the probabilities of texts in the testing dataset\n",
        "probs = mistral_inference.inference(test_df['text'].tolist())\n",
        "\n",
        "IDs = test_df['id'].values\n",
        "predictions = []\n",
        "for ID, prob in zip(IDs, probs):\n",
        "    print(f\"ID {ID}, prob = {prob}\")\n",
        "    predictions.append({'id': ID, 'generated': prob})\n",
        "print(f\"predictions = {predictions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d833ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:09.653157Z",
          "iopub.status.busy": "2024-01-22T18:57:09.652866Z",
          "iopub.status.idle": "2024-01-22T18:57:09.918345Z",
          "shell.execute_reply": "2024-01-22T18:57:09.917470Z"
        },
        "papermill": {
          "duration": 0.277972,
          "end_time": "2024-01-22T18:57:09.920260",
          "exception": false,
          "start_time": "2024-01-22T18:57:09.642288",
          "status": "completed"
        },
        "tags": [],
        "id": "b8d833ff",
        "outputId": "6e3e1de7-80a1-445f-9dce-040e6d0c1bde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mistral_res = pd.DataFrame(predictions)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e662280b",
      "metadata": {
        "papermill": {
          "duration": 0.009455,
          "end_time": "2024-01-22T18:57:09.939483",
          "exception": false,
          "start_time": "2024-01-22T18:57:09.930028",
          "status": "completed"
        },
        "tags": [],
        "id": "e662280b"
      },
      "source": [
        "### distilroberta + deberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2812c7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:09.959965Z",
          "iopub.status.busy": "2024-01-22T18:57:09.959391Z",
          "iopub.status.idle": "2024-01-22T18:57:09.964598Z",
          "shell.execute_reply": "2024-01-22T18:57:09.963607Z"
        },
        "papermill": {
          "duration": 0.017338,
          "end_time": "2024-01-22T18:57:09.966489",
          "exception": false,
          "start_time": "2024-01-22T18:57:09.949151",
          "status": "completed"
        },
        "tags": [],
        "id": "eb2812c7"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03dccd21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:09.986221Z",
          "iopub.status.busy": "2024-01-22T18:57:09.985950Z",
          "iopub.status.idle": "2024-01-22T18:57:14.157230Z",
          "shell.execute_reply": "2024-01-22T18:57:14.156085Z"
        },
        "papermill": {
          "duration": 4.184291,
          "end_time": "2024-01-22T18:57:14.160084",
          "exception": false,
          "start_time": "2024-01-22T18:57:09.975793",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "b04c1bebe64742ba87e6882f6beede29"
          ]
        },
        "id": "03dccd21",
        "outputId": "b9cb3da1-92cc-4c52-8394-e1ded70999bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b04c1bebe64742ba87e6882f6beede29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000aaaa</td>\n",
              "      <td>0.001225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1111bbbb</td>\n",
              "      <td>0.001146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2222cccc</td>\n",
              "      <td>0.001189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  generated\n",
              "0  0000aaaa   0.001225\n",
              "1  1111bbbb   0.001146\n",
              "2  2222cccc   0.001189"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_checkpoint = \"/detect-llm-models/distilroberta-finetuned_v5/checkpoint-49654\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], max_length = 512 , padding=True, truncation=True)\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Move your model and data to the GPU\n",
        "model.to(device);\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "test = pd.read_csv('/llm-detect-ai-generated-text/test_essays.csv').sort_values('id')\n",
        "test_ds = Dataset.from_pandas(test)\n",
        "test_ds_enc = test_ds.map(preprocess_function, batched=True)\n",
        "test_preds = trainer.predict(test_ds_enc)\n",
        "logits = test_preds.predictions\n",
        "probs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,0]\n",
        "res = pd.DataFrame()\n",
        "res['id'] = test['id']\n",
        "res['generated'] = probs\n",
        "res = res.sort_values('id')\n",
        "res.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4515b08",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:14.186838Z",
          "iopub.status.busy": "2024-01-22T18:57:14.186527Z",
          "iopub.status.idle": "2024-01-22T18:57:14.486817Z",
          "shell.execute_reply": "2024-01-22T18:57:14.485889Z"
        },
        "papermill": {
          "duration": 0.315981,
          "end_time": "2024-01-22T18:57:14.489261",
          "exception": false,
          "start_time": "2024-01-22T18:57:14.173280",
          "status": "completed"
        },
        "tags": [],
        "id": "e4515b08"
      },
      "outputs": [],
      "source": [
        "del model, trainer, test_ds, test_ds_enc, tokenizer, test_preds, Trainer, TrainingArguments\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9cc6a3",
      "metadata": {
        "papermill": {
          "duration": 0.009932,
          "end_time": "2024-01-22T18:57:14.509447",
          "exception": false,
          "start_time": "2024-01-22T18:57:14.499515",
          "status": "completed"
        },
        "tags": [],
        "id": "db9cc6a3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d37400",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:14.530482Z",
          "iopub.status.busy": "2024-01-22T18:57:14.530166Z",
          "iopub.status.idle": "2024-01-22T18:57:14.549270Z",
          "shell.execute_reply": "2024-01-22T18:57:14.548359Z"
        },
        "papermill": {
          "duration": 0.031806,
          "end_time": "2024-01-22T18:57:14.551121",
          "exception": false,
          "start_time": "2024-01-22T18:57:14.519315",
          "status": "completed"
        },
        "tags": [],
        "id": "61d37400"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "df = pd.read_csv(\"/llm-detect-ai-generated-text/test_essays.csv\").sort_values('id')\n",
        "dataset = Dataset.from_pandas(df[[\"text\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40ea5ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:14.572343Z",
          "iopub.status.busy": "2024-01-22T18:57:14.572062Z",
          "iopub.status.idle": "2024-01-22T18:57:15.779712Z",
          "shell.execute_reply": "2024-01-22T18:57:15.778665Z"
        },
        "papermill": {
          "duration": 1.220644,
          "end_time": "2024-01-22T18:57:15.781656",
          "exception": false,
          "start_time": "2024-01-22T18:57:14.561012",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "eaf665cae22d4fa68f1df320591bf744"
          ]
        },
        "id": "f40ea5ea",
        "outputId": "39b37413-0733-4bbe-9a20-be9f7b935390"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaf665cae22d4fa68f1df320591bf744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('/huggingfacedebertav3variants/deberta-v3-small')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokz = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    return tokz\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ee47b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:15.803531Z",
          "iopub.status.busy": "2024-01-22T18:57:15.803217Z",
          "iopub.status.idle": "2024-01-22T18:57:22.204401Z",
          "shell.execute_reply": "2024-01-22T18:57:22.203490Z"
        },
        "papermill": {
          "duration": 6.414811,
          "end_time": "2024-01-22T18:57:22.206935",
          "exception": false,
          "start_time": "2024-01-22T18:57:15.792124",
          "status": "completed"
        },
        "tags": [],
        "id": "05ee47b5"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('/debertav3-small-llm-trained/checkpoint-24579-20240118T191843Z-001/checkpoint-24579')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c3391a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.230102Z",
          "iopub.status.busy": "2024-01-22T18:57:22.229818Z",
          "iopub.status.idle": "2024-01-22T18:57:22.544067Z",
          "shell.execute_reply": "2024-01-22T18:57:22.543185Z"
        },
        "papermill": {
          "duration": 0.328249,
          "end_time": "2024-01-22T18:57:22.546087",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.217838",
          "status": "completed"
        },
        "tags": [],
        "id": "59c3391a",
        "outputId": "e6aa9d0e-68c3-4402-c039-8dded744deb8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_args = TrainingArguments(\n",
        "    output_dir = 'deberta-small',\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    dataloader_drop_last = False\n",
        ")\n",
        "\n",
        "# init trainer\n",
        "trainer = Trainer(\n",
        "              model = model,\n",
        "              args = test_args)\n",
        "\n",
        "test_results = trainer.predict(tokenized_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf78dfc4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.568718Z",
          "iopub.status.busy": "2024-01-22T18:57:22.568405Z",
          "iopub.status.idle": "2024-01-22T18:57:22.573356Z",
          "shell.execute_reply": "2024-01-22T18:57:22.572654Z"
        },
        "papermill": {
          "duration": 0.0184,
          "end_time": "2024-01-22T18:57:22.575309",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.556909",
          "status": "completed"
        },
        "tags": [],
        "id": "bf78dfc4"
      },
      "outputs": [],
      "source": [
        "probas = torch.nn.functional.softmax(torch.from_numpy(test_results.predictions), dim=1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa31fa40",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.639071Z",
          "iopub.status.busy": "2024-01-22T18:57:22.638766Z",
          "iopub.status.idle": "2024-01-22T18:57:22.645232Z",
          "shell.execute_reply": "2024-01-22T18:57:22.644405Z"
        },
        "papermill": {
          "duration": 0.061598,
          "end_time": "2024-01-22T18:57:22.647158",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.585560",
          "status": "completed"
        },
        "tags": [],
        "id": "aa31fa40"
      },
      "outputs": [],
      "source": [
        "deb_res = pd.DataFrame()\n",
        "deb_res['generated'] = probas.T[1]\n",
        "deb_res['id'] = df['id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db11ada",
      "metadata": {
        "papermill": {
          "duration": 0.010203,
          "end_time": "2024-01-22T18:57:22.668255",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.658052",
          "status": "completed"
        },
        "tags": [],
        "id": "7db11ada"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642f7a88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.690072Z",
          "iopub.status.busy": "2024-01-22T18:57:22.689810Z",
          "iopub.status.idle": "2024-01-22T18:57:22.696734Z",
          "shell.execute_reply": "2024-01-22T18:57:22.695867Z"
        },
        "papermill": {
          "duration": 0.020308,
          "end_time": "2024-01-22T18:57:22.698869",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.678561",
          "status": "completed"
        },
        "tags": [],
        "id": "642f7a88"
      },
      "outputs": [],
      "source": [
        "WEIGHTS = [0.30, 0.35, 0.25]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': test[\"id\"],\n",
        "    'generated': mistral_res['generated'] * WEIGHTS[0] + res['generated'] * WEIGHTS[1] + deb_res['generated'] * WEIGHTS[2]\n",
        "})\n",
        "submission = submission.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fca2c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.720843Z",
          "iopub.status.busy": "2024-01-22T18:57:22.720494Z",
          "iopub.status.idle": "2024-01-22T18:57:22.738303Z",
          "shell.execute_reply": "2024-01-22T18:57:22.737453Z"
        },
        "papermill": {
          "duration": 0.031034,
          "end_time": "2024-01-22T18:57:22.740242",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.709208",
          "status": "completed"
        },
        "tags": [],
        "id": "32fca2c9"
      },
      "outputs": [],
      "source": [
        "hkw = ['because', 'then', 'dont', 'texting', 'probably', 'almost']\n",
        "mkw = ['additionally', 'significant', 'attitude', 'failure', 'climate', 'ensures', 'address', 'achieving', 'graduating', 'engagement', 'determination', 'impression', 'drawbacks', 'modes', 'enthusiasm', 'kindness', 'prioritize', 'urban', 'commitments', 'embrace', 'reliance', 'supportive', 'fulfilling', 'stricter', 'adopting', 'argues', 'conservation', 'gun', 'artificial', 'violent', 'foster', 'failures', 'initial', 'employers', 'stability', 'meat', 'monitoring', 'aim', 'libraries', 'geological', 'committing', 'external', 'maintenance', 'footprint', 'undemocratic', 'platforms', 'consumption', 'shaping', 'biases', 'highlights', 'invaluable', 'societal', 'infrastructure', 'integral', 'diet', 'populous', 'insights', 'chronic', 'profound', 'sustainable', 'livable', 'internships', 'constitutional', 'setbacks', 'codes', 'inclusive', 'align', 'successes', 'delays', 'densely', 'marine', 'ethical', 'belonging', 'guns', 'addressing', 'appreciation', 'trump', 'smith', 'fosters', 'wage', 'firsthand', 'cyberbullying', 'emphasizes', 'embracing']\n",
        "\n",
        "def count_keywords(text, keywords):\n",
        "    count = 0\n",
        "    ltext = text.lower()\n",
        "    for kw in keywords:\n",
        "        count += (kw in ltext)\n",
        "\n",
        "    return count\n",
        "\n",
        "prob_df = pd.DataFrame()\n",
        "df[\"h_keyword_count\"] = df['text'].apply(count_keywords, keywords = hkw)\n",
        "df[\"m_keyword_count\"] = df['text'].apply(count_keywords, keywords = mkw)\n",
        "prob_df = submission\n",
        "prob_df['new_prob'] = np.where(prob_df['generated'] <= prob_df['generated'].quantile(0.50),\n",
        "                               prob_df['generated'] - df['h_keyword_count'] * (prob_df['generated'] * 0.10), prob_df['generated'])\n",
        "prob_df['new_prob'] = np.where(prob_df['generated'] > prob_df['generated'].quantile(0.50),\n",
        "                               prob_df['generated'] + df['m_keyword_count'] * (prob_df['generated'] * 0.10), prob_df['new_prob'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc711861",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.762772Z",
          "iopub.status.busy": "2024-01-22T18:57:22.762491Z",
          "iopub.status.idle": "2024-01-22T18:57:22.766714Z",
          "shell.execute_reply": "2024-01-22T18:57:22.765858Z"
        },
        "papermill": {
          "duration": 0.017469,
          "end_time": "2024-01-22T18:57:22.768685",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.751216",
          "status": "completed"
        },
        "tags": [],
        "id": "cc711861"
      },
      "outputs": [],
      "source": [
        "prob_df['generated'] = prob_df['new_prob']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43703601",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.790033Z",
          "iopub.status.busy": "2024-01-22T18:57:22.789781Z",
          "iopub.status.idle": "2024-01-22T18:57:22.798266Z",
          "shell.execute_reply": "2024-01-22T18:57:22.797400Z"
        },
        "papermill": {
          "duration": 0.021322,
          "end_time": "2024-01-22T18:57:22.800186",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.778864",
          "status": "completed"
        },
        "tags": [],
        "id": "43703601",
        "outputId": "1281cc07-7950-44b5-db9e-eaad46f031ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000aaaa</td>\n",
              "      <td>0.549987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1111bbbb</td>\n",
              "      <td>0.493563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2222cccc</td>\n",
              "      <td>0.550219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  generated\n",
              "0  0000aaaa   0.549987\n",
              "1  1111bbbb   0.493563\n",
              "2  2222cccc   0.550219"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del prob_df['new_prob']\n",
        "prob_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6768a202",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-22T18:57:22.822388Z",
          "iopub.status.busy": "2024-01-22T18:57:22.822129Z",
          "iopub.status.idle": "2024-01-22T18:57:22.828834Z",
          "shell.execute_reply": "2024-01-22T18:57:22.827870Z"
        },
        "papermill": {
          "duration": 0.020218,
          "end_time": "2024-01-22T18:57:22.831167",
          "exception": false,
          "start_time": "2024-01-22T18:57:22.810949",
          "status": "completed"
        },
        "tags": [],
        "id": "6768a202"
      },
      "outputs": [],
      "source": [
        "prob_df.to_csv('sheet.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 7516023,
          "sourceId": 61542,
          "sourceType": "competition"
        },
        {
          "datasetId": 1335671,
          "sourceId": 2233309,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2663421,
          "sourceId": 4620664,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3972872,
          "sourceId": 6921012,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4005256,
          "sourceId": 6977472,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4257160,
          "sourceId": 7333485,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3954249,
          "sourceId": 7407169,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4323916,
          "sourceId": 7431021,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4334232,
          "sourceId": 7450405,
          "sourceType": "datasetVersion"
        },
        {
          "modelInstanceId": 3899,
          "sourceId": 5111,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 328.658438,
      "end_time": "2024-01-22T18:57:25.762835",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-01-22T18:51:57.104397",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}